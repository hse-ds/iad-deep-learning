{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmJo7IfF_58i"
   },
   "source": [
    "Скачаем и рахархивируем заранее все данные, которые понадобятся на семинаре + установим недостающий модуль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MY2kFBuI_uK_",
    "outputId": "d7e73f24-db48-438e-d684-b07b34cb3865"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/tc1qo73rrm3gt3m/CARVANA.zip  # Carvana dataset\n",
    "!wget https://www.dropbox.com/s/k886cusbuc1afnq/imagenet-mini.zip  # mini image-net dataset\n",
    "!wget https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt  # классы имаджнета\n",
    "!unzip -q CARVANA.zip\n",
    "!unzip -q imagenet-mini.zip\n",
    "!rm -rf ./train/.DS_Store\n",
    "!rm -rf ./train_masks/.DS_Store\n",
    "!pip install colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7upRqguxAo90",
    "outputId": "169b4058-db96-443a-db2f-56a9cb92964a"
   },
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQv54rfdELFv"
   },
   "source": [
    "<h2>Часть 1. mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBJgNh0WELFv"
   },
   "source": [
    "Для определения метрики mean Average Precision понадобятся проделать небольшой путь и вспомнить пару понятий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHuMyUHKELFw"
   },
   "source": [
    "Вспоминаем второй класс ИАДа:\n",
    "<br>\n",
    "$$ Precision = {TP \\over TP + FP} $$\n",
    "<br>\n",
    "$$ Recall = {TP \\over TP + FN} $$\n",
    "<br>\n",
    "<br>TP - True Positive\n",
    "<br>FP - False Positive\n",
    "<br>FN - False Negative\n",
    "<br> <br> __Precision__ - доля объектов, названных классификатором положительными и при этом действительно являющимися положительными\n",
    "\n",
    "__Recall__ - показывает, какую долю объектов положительного класса из всех объектов положительного класса нашел алгоритм. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLB8y0--ELFx"
   },
   "source": [
    "<h4>Вспоминаем лекцию 6:\n",
    "Intersection over Union</h4>\n",
    "<img src=https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI5Y3JDlELFx"
   },
   "source": [
    "В задаче детекции метки TP, FP, FN (чаще всего) выдаются по следующей логике\n",
    "- метка TP выдается в случае если IoU > 0.5 и класс определен правильно\n",
    "- метка FP выдается в случае если IoU <= 0.5 и/или если Bounding Box'ы дублируются\n",
    "- метка FN выдается если если IoU > 0.5, но неправильно определен класс и/или если бокса нет совсем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQqzHZmwTTT3"
   },
   "source": [
    "<h3> Чтобы было нагляднее: </h3>\n",
    "* зеленый цвет - истинный бокс и класс, синий - наши предсказания\n",
    "<br> <h3>True Positive </h3> У нас два волка, оба определены своим классом и боксы очевидно имеют IoU больше 0.5 </br>\n",
    "<br>\n",
    "<img src=\"TruePositiveVolks.jpg\">\n",
    "\n",
    "<br> <h3>False Positive </h3> Несмотря на то, что класс определен правильно и бокс в целом выглядит логично на своем месте, IoU слишком мал, поэтому такая детекция получает метку FP </br>\n",
    "<br>\n",
    "<img src=\"FalsePositiveVolk.jpg\">\n",
    "\n",
    "<br> <h3>False Negative </h3> Потому что несмотря на хорошее пересечение предсказанного бокса с целевым, класс с высокой уверенностью определен неправильно </br>\n",
    "<br>\n",
    "<img src=\"FalseNegativeVolk.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8TbTIshELFx"
   },
   "source": [
    "Далее, нужно построить PR-кривую (Напоминание: это кривая, у которой по оси Y - значение Precision, а по оси X - значение recall. Эти значения считаются при переборе пороговых вероятностей, начиная с которых объект помечается положительным классом)\n",
    "<br><br> Для задачи бинарной классификации мы когда-то строили такую кривую, теперь рассмотрим чуть более сложный случай, где у нас три класса: Волк, Лев и Тигр + решается задача детекции, а не классификации.\n",
    "<br><br> Внизу представлена таблица с игрушечными данными по предсказаниям модели. \n",
    "<br><br>Допустим из 7 объектов, в датасете у нас только 3 волка. В данном случае мы называли объект \"действительно волком\" если он имел правильный класс и IoU не менее 0.5. Таким образом имеем задачу вида one vs all, где интересующим нас классом будет являться именно \"волк\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwawmoyeELFy"
   },
   "source": [
    "|Номер строки| Уверенность в том что волк (истинный класс)    |IoU не менее 0.5?   |  Precision  |  Recall |\n",
    "|------------|------------------------|----------------------|-------------|---------| \n",
    "|1           |0.92 (Волк)     |   True    |    1.0     |    0.33 |\n",
    "|2           |0.83 (Волк)     |   True    |    1.0     |    0.67 | \n",
    "|3           |0.77 (Волк)     |   False   |    0.67    |    0.67 | \n",
    "|4           |0.71 (Лев)      |   False   |    0.50    |    ...  |\n",
    "|5           |0.67 (Тигр)     |   False   |    0.40    |    .... | \n",
    "|6           |0.54 (Волк)     |   True    |    0.50    |    .... | \n",
    "|7           |0.47 (Тигр)     |   False   |    0.50    |    1.0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehDXMLbjELFy"
   },
   "source": [
    "<h6>Посчитаем Precision и Recall для порога в 0.9:</h6>\n",
    "<br> Здесь все легко, взяли порог в 0.9 и называем волками всех, у кого уверенность в классе \"Волк\" больше 0.9. Один TP, отсутсвуют FP и два FN (2 и 6 строчки). \n",
    "<br> $$Precision ={  1 \\over 1 + 0} = 1.0$$\n",
    "<br> $$Recall = {1 \\over 1 + 2} = 0.33$$\n",
    "\n",
    "<h6>для порога в 0.8:</h6> \n",
    "<br> Здесь тоже без дополнительных сложностей. С таким порогом, во второй строчке у нас нашелся еще один TP, соотоветственно убавился один FN. Остальное осталось так же\n",
    "<br> $$Precision ={  2 \\over 2 + 0} = 1.0$$\n",
    "<br> $$Recall = {2 \\over 2 + 1} = 0.67$$\n",
    "\n",
    "<h6>для порога в 0.75:</h6>  \n",
    "<br> А вот при пороге в 0.75 в третьей строчке замечаем, что несмотря на то, что истинный класс действительно \"Волк\", чем мы и называем данный объект - IoU c истинным боксом меньше 0.5, поэтому присваиваем метку FP.\n",
    "<br> $$Precision ={  2 \\over 2 + 1} = 0.67$$\n",
    "<br> $$Recall = {2 \\over 2 + 1} = 0.67$$\n",
    "\n",
    "<br> <br> <h6>Дальше сами :)</h6>\n",
    "<br> <h6>Задание: Посчитать недостающие в таблице значения precision и recall и сравнить с изображенной PR кривой</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9uzsBm5ELFy"
   },
   "source": [
    "PR кривая будет вылядеть следуюшим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIE-taVs_iUL"
   },
   "source": [
    "<img src=\"pr_uno.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62fZzJWCELF4"
   },
   "source": [
    "Average Precision (AP) стандартно определяется как AUC-PR, то есть как площадь по PR кривой. Из-за того, что Precision и Recall находятся в отрезке от 0 до 1, AP так же определена на этом отрезке. Чем ближе к 1, тем качественнее модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J5NSMO7ELF5"
   },
   "source": [
    "* Для удобства вычислений, и чуть большей устойчивости к перестановке - вместо того, чтобы терпеть возникшую немонотонность, для всех совпадающих значений recall'a берется максимальный справа от текущей точки precision, то есть график изменится следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhva_Z_T_iUM"
   },
   "source": [
    "\n",
    "<img src=\"pr_dos.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56hods6PELF8"
   },
   "source": [
    "Технология та же - для вычисления AP считается AUC под красной кривой\n",
    "<br> $$AP = 1 * 0.67 + (1 - 0.67) * 0.5 = 0.835 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlQ8t0jMELF8"
   },
   "source": [
    "* <br>В какой-то момент люди решили, что просто площади теперь не круто и в популярном соревновании PASCAL VOC2008 для вычисления Average Precision использовалась 11-point interpolation. \n",
    "<br>По-простому: брались 11 значений монотонной PR функции, в точках 0, 0.1, 0.2, ..., 0.9, 1.0 и усреднялись. <br>Для любителей формул:\n",
    "\n",
    "<br> $$AP = {1 \\over11} * \\sum P(r), r \\in [0.1, 0.2, ..., 0.9, 1.0]$$\n",
    "<br> P(r) - значение Precision при определенном Recall\n",
    "<br> \n",
    "<br> графически это все выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKcQmGxa_iUO"
   },
   "source": [
    "\n",
    "<img src=\"pr_tres.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEOlVyTs_iUO"
   },
   "source": [
    "Посчитаем АР данным способом:\n",
    "<br> $$AP = {1 \\over {11}} (1 * 7 + 3 * 0.5) = 0.77 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuOvwsGsELF-"
   },
   "source": [
    "* Для более <font color=\"green\">fresh </font> PASCAl VOC соревнований (2011 - 2012) было принято решение считать Average Precision как обычную __площадь под монотонной PR кривой__. Этого определения мы и будем придерживаться далее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dN1KII-ELF_"
   },
   "source": [
    "Все предыдущие графики, наша первая табличка и значения AP считались для одного класса \"Волк\". Понятно, что подобные значения можно посчитать для каждого класса в выборке. И каждый раз будет принцип one vs all, где различается нужный класс и \"все остальные\"\n",
    "<br><br>Метрика __mean Average Precison__ считается как среднее между __AP__ каждого класса, те:\n",
    "\n",
    "$$mAP = \\sum_{c \\in C} AP(c)$$\n",
    "где С - множество классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7UlJvsdELF_"
   },
   "source": [
    "Ниже - пример того как считать среднее :)) \n",
    "<br>*для Average Precision льва и тигра значения взяты случайно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKn0283yELF_"
   },
   "source": [
    "|AP(Волк)| AP(Лев) | AP(Тигр)|mAp    |\n",
    "|--------|---------|---------|-------|\n",
    "|0.835   |0.83     | 0.77    | 0.81  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQAG0kBGELF_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubc0wbOl_iUQ"
   },
   "source": [
    "<h2>Часть 2. Semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq_ppf6t_iUQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "from colour import Color\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ff_VyjsEaym"
   },
   "source": [
    "<h> В этой части семинара мы напишем свой U-net с нуля, без мам, пап и кредитов,\n",
    "обучим и попробуем его на задачу распознавания точной маски автомобиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_jCFjB2_iUT"
   },
   "outputs": [],
   "source": [
    "class Carvana(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        transform=None,\n",
    "        ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param root: путь к папке с данными\n",
    "        :param transform: transform the images and labels\n",
    "        \"\"\"\n",
    "\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        (self.data_path, self.labels_path) = ([], [])\n",
    "\n",
    "        def load_images(path):\n",
    "            \"\"\"\n",
    "            returns all the sorted image paths.\n",
    "\n",
    "            :param path:\n",
    "            :return: лист с путями до всех изображений\n",
    "            \"\"\"\n",
    "\n",
    "            images_dir = [join(path, f) for f in os.listdir(path)\n",
    "                          if isfile(join(path, f))]\n",
    "            images_dir.sort()\n",
    "\n",
    "            return images_dir\n",
    "\n",
    "        self.data_path = load_images(self.root + '/train')\n",
    "        self.labels_path = load_images(self.root + '/train_masks')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "\n",
    "        :param index:\n",
    "        :return: tuple (img, target) with the input data and its label\n",
    "        \"\"\"\n",
    "\n",
    "        img = Image.open(self.data_path[index])\n",
    "        target = Image.open(self.labels_path[index])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            target = self.transform(target)\n",
    "            target = (target > 0).float()\n",
    "\n",
    "        return (img, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "def im_show(img_list):\n",
    "    \"\"\"\n",
    "    It receives a list of images and plots them together\n",
    "    :param img_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    to_PIL = transforms.ToPILImage()\n",
    "    if len(img_list) > 9:\n",
    "        raise Exception('len(img_list) must be smaller than 10')\n",
    "\n",
    "    for (idx, img) in enumerate(img_list):\n",
    "        img = np.array(to_PIL(img))\n",
    "        # plt.subplot(100 + 10 * len(img_list) + idx + 1)\n",
    "        plt.figure(figsize=(4,4))\n",
    "        fig = plt.imshow(img)\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLiA3F2SMQiA"
   },
   "source": [
    "Загрузим датасет при помощи определенного выше класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVBdhAWMLlzy"
   },
   "outputs": [],
   "source": [
    "train_dataset = Carvana(root='.',\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize((256, 256)),\n",
    "                            transforms.ToTensor()])\n",
    "                        )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKUcQZpLMTy_"
   },
   "source": [
    "Посмотрим несколько примеров изображений и масок автомобиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2VIj0iLOLou-",
    "outputId": "87886f75-29c0-4bee-8902-a6001cfab05b"
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for i in range(4):\n",
    "    img, label = train_dataset[np.random.randint(0, 100)]\n",
    "    img_list.append(img)\n",
    "    img_list.append(label)\n",
    "\n",
    "im_show(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmRQhwT-t8CR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKLjoUgsLqr1"
   },
   "source": [
    "Дополните недостающий код таким образом, чтобы получилась архитектура U-net (https://arxiv.org/pdf/1505.04597.pdf) сети. Обратите внимание, что при проходе \"вниз\" количество каналов каждого блока __увеличивается в два раза__. Ситуация с проходом \"вверх\" противоположна, количество каналлов __уменьшается вдвое__. Также, начинаем мы __не с 64 каналов__, как на схеме ниже, __а с 16 каналов__. \n",
    "<br>При проходе вниз в нашей реализации предлагается __дойти до 128 каналов__, чтобы сэкономить время обучения, в оригинальной статье было до 1024. (2 блока, вместо 4х)\n",
    "<br>В целом, при отличии количества каналлов от указанных выше чисел - модель все равно будет работать, но для удобства проведения семинара лучше всем договориться об одних числах\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\">\n",
    "<br> Подсказка: каждый блок по пути \"вниз\" представляет из себя двойную свертку с батч нормом и активацией (при создании Relu слоя не забывайте inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-8ltVJe_iUV"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, padding=1, stride=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size,  # <YOUR CODE HERE>\n",
    "                              padding=padding, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_size)\n",
    "        self.relu = nn.ReLU(inplace=True) # <YOUR CODE HERE>\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        self.down_1 = nn.Sequential(\n",
    "            ConvBlock(3, 16),\n",
    "            ConvBlock(16, 32, stride=2, padding=1))\n",
    "\n",
    "        self.down_2 = nn.Sequential(  # <YOUR CODE HERE>\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128))\n",
    "\n",
    "        self.middle = ConvBlock(128, 128, kernel_size=1, padding=0)\n",
    "\n",
    "        self.up_2 = nn.Sequential(  # <YOUR CODE HERE>\n",
    "            ConvBlock(256, 128),\n",
    "            ConvBlock(128, 32))\n",
    "\n",
    "        self.up_1 = nn.Sequential(  # <YOUR CODE HERE>\n",
    "            ConvBlock(64, 64),\n",
    "            ConvBlock(64, 32))\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            ConvBlock(32, 16),\n",
    "            ConvBlock(16, 1, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        down1 = self.down_1(x)  # <YOUR CODE HERE>\n",
    "        out = F.max_pool2d(down1, kernel_size=2, stride=2)\n",
    "\n",
    "        down2 = self.down_2(out)  # <YOUR CODE HERE>\n",
    "        out = F.max_pool2d(down2, kernel_size=2, stride=2)  # <YOUR CODE HERE>\n",
    "\n",
    "        out = self.middle(out)\n",
    "\n",
    "        out = nn.functional.interpolate(out, scale_factor=2)  # интерполяцией увеличиваем размер фильтра вдвое\n",
    "        out = torch.cat([down2, out], 1)\n",
    "        out = self.up_2(out)\n",
    "\n",
    "        out = nn.functional.interpolate(out, scale_factor=2)  # <YOUR CODE HERE>\n",
    "        out = torch.cat([down1, out], 1)   # <YOUR CODE HERE>\n",
    "        out = self.up_1(out)   # <YOUR CODE HERE>\n",
    "\n",
    "        out = nn.functional.interpolate(out, scale_factor=2)\n",
    "        return self.output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE58rn_ZMohS"
   },
   "source": [
    "Определяем функцию для обучения одной эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5ZF0LeS_iUb"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, epoch, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = torch.sigmoid(model(images))\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        accuracy = ((outputs > 0.5) == labels).float().mean()\n",
    "            \n",
    "        pbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)} \"\n",
    "                f\"Accuracy: {round(accuracy.item() * 100, 4)}\"\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RnYf1lS_iUd",
    "outputId": "4d929f03-4843-4634-95d4-c29a105d0119"
   },
   "outputs": [],
   "source": [
    "model = Unet().cuda()\n",
    "criterion = torch.nn.BCELoss().cuda()  # https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                                weight_decay=1e-4,\n",
    "                                lr=1e-4,\n",
    "                                momentum=0.9)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(0, num_epochs):\n",
    "    train(train_loader, model, criterion, epoch, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G07YWsb2FDff"
   },
   "source": [
    "Посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5aytNIaaDa4S",
    "outputId": "757aa449-0c67-4f8e-ba69-c460902f374d"
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for i in range(4):\n",
    "    img, label = train_dataset[np.random.randint(0, 100)]\n",
    "    img_list.append(img)\n",
    "    img_list.append(model(img.unsqueeze(0).cuda()).squeeze(0).cpu())\n",
    "\n",
    "im_show(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVvff-qgkdvT"
   },
   "source": [
    "Еще несколько эпох и никакие гринскрины нужны не будут..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjnzaZ9vNqX7"
   },
   "outputs": [],
   "source": [
    "#освободим память\n",
    "model.cpu()\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr09Rv6F_iUf"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiIqDhk_ELGA"
   },
   "source": [
    "<h2>Часть 3. Детекция изображений</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня на семинаре мы попробуем обучить детекцию. Мы не будем полностью реализовывать архитектуру сети, а возьмем предобученную Faster R-CNN из pytorch и сделаем fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icjhclUNIgCk"
   },
   "source": [
    "Для начала загрузим данные. Мы будем пользоваться датасетом [Penn-Fudan](https://www.cis.upenn.edu/~jshi/ped_html/). Он содержит 170 изображений с разметкой сегментации и детекции. Мы воспользуемся последней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaGQea5hIgCn",
    "outputId": "2b25078d-617d-4ccc-cec2-cd4ac140d703"
   },
   "outputs": [],
   "source": [
    "! curl https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip > PennFudanPed.zip\n",
    "! unzip -q PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtNWxqoMESqw"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1g32XYFFYTb"
   },
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": image_id}\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqys9Lf9-PAQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transform(train=False):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYRhV4hfGpq9"
   },
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('PennFudanPed/', get_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "2PJGAZYs6mQE",
    "outputId": "1e91df1b-127b-43ca-c387-4639ac092220"
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "image, labels = next(iter(dataset))\n",
    "\n",
    "image = T.ToPILImage()(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in labels['boxes']:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XytZV2QIgDC"
   },
   "source": [
    "Теперь подгрузим модель. Среди моделей детекции в pytorch есть только Faster R-CNN. Так как мы детектим только 2 класса - пешеходов и фон, нужно изменить выходной слой, предсказывающий классы изображений. Для этого в torchvision есть блок FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VABmgMQ3IgDI"
   },
   "source": [
    "Fast R-CNN</h4>\n",
    "<img src=https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSUjeN0M7EWk"
   },
   "outputs": [],
   "source": [
    "def get_detection_model(num_classes=2):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdrRAZc7IgDO"
   },
   "source": [
    "Теперь давайте напишем подсчет IoU, которая пригодится нам для отбора кандидатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBvGLhxOIgDP"
   },
   "source": [
    "Intersection over Union</h4>\n",
    "<img src=https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx3qsM_fIgDP"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(dt_bbox, gt_bbox):\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "    ## TODO YOUR CODE\n",
    "    intersection_box = np.array([\n",
    "      max(dt_bbox[0],gt_bbox[0]), # x_min 2\n",
    "      max(dt_bbox[1],gt_bbox[1]), # y_min 1\n",
    "      min(dt_bbox[2],gt_bbox[2]), # x_max 2\n",
    "      min(dt_bbox[3],gt_bbox[3]), # y_max 1\n",
    "\n",
    "    ])\n",
    "    intersection_area = max(intersection_box[2] - intersection_box[0], 0) * max(intersection_box[3] - intersection_box[1], 0)\n",
    "    dt_area = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
    "    gt_area = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "    iou = intersection_area / (dt_area + gt_area - intersection_area)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mV4wTZpMNotf",
    "outputId": "01e21e5b-c601-4bae-aaf8-b7b69d8faf91"
   },
   "outputs": [],
   "source": [
    "dt_bbox = [0, 0, 2, 2]\n",
    "gt_bbox = [1, 1, 3, 3]\n",
    "intersection_over_union(dt_bbox, gt_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Pkva3QFDDed"
   },
   "outputs": [],
   "source": [
    "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
    "    # правильные прямоугольники\n",
    "    gt_bboxes = target_true['boxes'].numpy()\n",
    "    gt_labels = target_true['labels'].numpy()\n",
    "\n",
    "    # предсказания модели\n",
    "    dt_bboxes = target_pred['boxes'].numpy()\n",
    "    dt_labels = target_pred['labels'].numpy()\n",
    "    dt_scores = target_pred['scores'].numpy()\n",
    "\n",
    "    results = []\n",
    "    # для каждого прямоугольника из предсказания находим максимально близкий прямоугольник среди ответов\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "\n",
    "        detection_result_dict = {'score': dt_score}\n",
    "\n",
    "        max_IoU = 0\n",
    "        max_gt_id = -1\n",
    "        for gt_id in range(len(gt_labels)):\n",
    "            gt_bbox = gt_bboxes[gt_id, :]\n",
    "            gt_label = gt_labels[gt_id]\n",
    "\n",
    "            if gt_label != dt_label:\n",
    "                continue\n",
    "\n",
    "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
    "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
    "                max_gt_id = gt_id\n",
    "\n",
    "        \n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            # для прямоугольника detection_id нашли правильный ответ, который имеет IoU больше 0.5 \n",
    "            detection_result_dict['TP'] = 1\n",
    "            # удаляем эти прямоугольники из данных, чтобы больше не матчить с ними\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict['TP'] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    # возвращаем результат, для кажого прямоугольника говорим, смогли ли сматчить его с чем то из ответов\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roHlrGtVIgDV"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, targets_true) in enumerate(test_loader):\n",
    "            images = list(image.to(device).float() for image in images)\n",
    "            targets_pred = model(images)\n",
    "\n",
    "            targets_true = [{k: v.cpu().float() for k, v in t.items()} for t in targets_true]\n",
    "            targets_pred = [{k: v.cpu().float() for k, v in t.items()} for t in targets_pred]\n",
    "\n",
    "            for i in range(len(targets_true)):\n",
    "                target_true = targets_true[i]\n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true['labels'].shape[0]\n",
    "\n",
    "                # матчим ответы с правильными боксами\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "\n",
    "    results = sorted(results, key=lambda k: k['score'], reverse=True)\n",
    "\n",
    "    # считаем точность и полноту, чтобы потом посчитать mAP как auc\n",
    "    acc_TP = np.zeros(len(results))\n",
    "    acc_FP = np.zeros(len(results))\n",
    "    recall = np.zeros(len(results))\n",
    "    precision = np.zeros(len(results))\n",
    "\n",
    "    if results[0]['TP'] == 1:\n",
    "        acc_TP[0] = 1\n",
    "    else:\n",
    "        acc_FP[0] = 1\n",
    "\n",
    "    for i in range(1, len(results)):\n",
    "        acc_TP[i] = results[i]['TP'] + acc_TP[i - 1]\n",
    "        acc_FP[i] = (1 - results[i]['TP']) + acc_FP[i - 1]\n",
    "\n",
    "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
    "        recall[i] = acc_TP[i] / nbr_boxes\n",
    "\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsqKh0gaIgDa"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    n = 0\n",
    "    global_loss = 0\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device).float() for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        dict_loss = model(images, targets)\n",
    "        losses = sum(loss for loss in dict_loss.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n += 1\n",
    "        global_loss += float(losses.cpu().detach().numpy())\n",
    "\n",
    "        if n % 10 == 0:\n",
    "            print(\"Loss value after {} batches is {}\".format(n, round(global_loss / n, 2)))\n",
    "\n",
    "    return global_loss\n",
    "\n",
    "\n",
    "def train(model, num_epochs, train_loader, test_loader, optimizer, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch {}/{}..\".format(epoch, num_epochs))\n",
    "        start = time.time()\n",
    "        train_one_epoch(model, optimizer, train_loader, device)\n",
    "        mAP = evaluate(model, test_loader, device=device)\n",
    "        end = time.time()\n",
    "\n",
    "        print(\"epoch {} done in {}s\".format(epoch, round(end - start, 2)))\n",
    "        print(\"mAP after epoch {} is {}:\".format(epoch, round(mAP, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQ9VxC0vIgDh"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEzKc4eR9BHn"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 2\n",
    "\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset_train = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[-50:])\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868,
     "referenced_widgets": [
      "0dff8103f53b4b38adb088af2f2d8956",
      "29bb1547ec9545ce85f339693c2af46b",
      "4f90da335c9a4cbf8d10fe1dc3c78813",
      "13277a01bbb94b6ca6c24d60bf29fcfd",
      "1788c9caf91b4c2bb8d2ef0d291f4b14",
      "7b7e261e948d4bfbb25df72521f25331",
      "12f7d44ba70b4b8fb9113164bb597fd9",
      "88b26b728294483681449ffba56cac49"
     ]
    },
    "id": "0vpF4zUbIgDt",
    "outputId": "ebbf2160-74bb-4d7e-dc56-7d8ad3d332aa"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model = get_detection_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "train(model, num_epochs, data_loader, data_loader_test, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0DlzqfsIgDv"
   },
   "source": [
    "Давайте посмотрим, как наша модель научилась выделять людей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq9JzMfiZ4wD"
   },
   "outputs": [],
   "source": [
    "image, labels = next(iter(dataset_test))\n",
    "pred = model(image.unsqueeze(0).to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "lZ8_iGQzIgDx",
    "outputId": "c7ec4729-846f-441b-9d45-8fa5eab2b4fd"
   },
   "outputs": [],
   "source": [
    "image = T.ToPILImage()(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in labels['boxes']:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "    \n",
    "for box in pred['boxes']:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qrj1cdioj3I3",
    "outputId": "82d982e3-5452-4633-a19e-24ac25f3f3ec"
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02VoP-2aMggf",
    "outputId": "c5ed9d3c-f23a-4c78-85dd-16c50ecae0b8"
   },
   "outputs": [],
   "source": [
    "intersection_over_union(pred['boxes'][0], pred['boxes'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sem_06_solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16fd80de06cf4fe7ad751626b0464edd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f64ca28e40485bbe05267742ff4a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16fd80de06cf4fe7ad751626b0464edd",
      "placeholder": "​",
      "style": "IPY_MODEL_55ce6e52c7c24dad89f4cdaa829f7a46",
      "value": " 507M/507M [00:06&lt;00:00, 76.7MB/s]"
     }
    },
    "3341c0402ee54b198d6e98c503db53bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "383292a7696a4ea6bd705a079b3d4a6e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55ce6e52c7c24dad89f4cdaa829f7a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66a33a75cd354566a10357147f425d50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7282de6cafa3432dbcb0c94cc438f32e",
       "IPY_MODEL_25f64ca28e40485bbe05267742ff4a57"
      ],
      "layout": "IPY_MODEL_383292a7696a4ea6bd705a079b3d4a6e"
     }
    },
    "7282de6cafa3432dbcb0c94cc438f32e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_845a8e8552fe43cd8907e2f0f1b53c9c",
      "max": 531503671,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3341c0402ee54b198d6e98c503db53bf",
      "value": 531503671
     }
    },
    "845a8e8552fe43cd8907e2f0f1b53c9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
