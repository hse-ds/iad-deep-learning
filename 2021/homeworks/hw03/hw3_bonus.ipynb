{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "colab": {
   "name": "hw3_bonus.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "212w0hi9bND_"
   },
   "source": [
    "# Домашнее задание 3. Бонус. Object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ustwuiG0bNEC"
   },
   "source": [
    "Вы можете получить за это задание до 5 баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANE0gO_KbNEJ"
   },
   "source": [
    "В этом задании потребуется обучить детектор фруктов на изображении. Датасет можно скачать [отсюда](https://yadi.sk/d/UPwQB7OZrB48qQ)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QOr4NI6XbNEK"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xYO7giUmbNEK"
   },
   "source": [
    "# we will need this library to process the labeling\n",
    "! pip install xmltodict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kdU4I0CxbNEK"
   },
   "source": [
    "import json\n",
    "\n",
    "import xmltodict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5bt-2SXbNEL"
   },
   "source": [
    "Датасет мы за вас написали."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QFeVZS0ebNEL"
   },
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import xmltodict\n",
    "from sklearn.metrics import auc\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# add any imports you need\n",
    "\n",
    "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
    "\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        self.transform = transform\n",
    "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
    "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
    "            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n",
    "            with open(annotation) as f:\n",
    "                annotation_dict = xmltodict.parse(f.read())\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
    "            if not isinstance(objects, list):\n",
    "                objects = [objects]\n",
    "            for obj in objects:\n",
    "                bndbox = obj[\"bndbox\"]\n",
    "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
    "                bbox = list(map(int, bbox))\n",
    "                bboxes.append(torch.tensor(bbox))\n",
    "                labels.append(class2tag[obj[\"name\"]])\n",
    "            self.annotations.append(\n",
    "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.transform:\n",
    "            # the following code is correct if you use albumentations\n",
    "            # if you use torchvision transforms you have to modify it =)\n",
    "            res = self.transform(\n",
    "                image=self.images[i],\n",
    "                bboxes=self.annotations[i][\"boxes\"],\n",
    "                labels=self.annotations[i][\"labels\"],\n",
    "            )\n",
    "            return res[\"image\"], {\n",
    "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
    "                \"labels\": torch.tensor(res[\"labels\"]),\n",
    "            }\n",
    "        else:\n",
    "            return self.images[i], self.annotations[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCtA2iM2bNEM"
   },
   "source": [
    "Выпишем кое-какую техническую работу, которая уже была на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xoO_p0KZbNEM"
   },
   "source": [
    "def intersection_over_union(dt_bbox, gt_bbox):\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO YOUR CODE\n",
    "\n",
    "    intersection_bbox = np.array(\n",
    "        [\n",
    "            max(dt_bbox[0], gt_bbox[0]),\n",
    "            max(dt_bbox[1], gt_bbox[1]),\n",
    "            min(dt_bbox[2], gt_bbox[2]),\n",
    "            min(dt_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
    "        intersection_bbox[3] - intersection_bbox[1], 0\n",
    "    )\n",
    "    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "\n",
    "\n",
    "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
    "    gt_bboxes = target_true[\"boxes\"].numpy()\n",
    "    gt_labels = target_true[\"labels\"].numpy()\n",
    "\n",
    "    dt_bboxes = target_pred[\"boxes\"].numpy()\n",
    "    dt_labels = target_pred[\"labels\"].numpy()\n",
    "    dt_scores = target_pred[\"scores\"].numpy()\n",
    "\n",
    "    results = []\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "\n",
    "        detection_result_dict = {\"score\": dt_score}\n",
    "\n",
    "        max_IoU = 0\n",
    "        max_gt_id = -1\n",
    "        for gt_id in range(len(gt_labels)):\n",
    "            gt_bbox = gt_bboxes[gt_id, :]\n",
    "            gt_label = gt_labels[gt_id]\n",
    "\n",
    "            if gt_label != dt_label:\n",
    "                continue\n",
    "\n",
    "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
    "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
    "                max_gt_id = gt_id\n",
    "\n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            detection_result_dict[\"TP\"] = 1\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict[\"TP\"] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, targets_true) in enumerate(test_loader):\n",
    "            images = list(image.to(device).float() for image in images)\n",
    "            targets_pred = model(images)\n",
    "            targets_true = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n",
    "            ]\n",
    "            targets_pred = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n",
    "            ]\n",
    "\n",
    "            for i in range(len(targets_true)):\n",
    "                target_true = targets_true[i]\n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
    "\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "\n",
    "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "    acc_TP = np.zeros(len(results))\n",
    "    acc_FP = np.zeros(len(results))\n",
    "    recall = np.zeros(len(results))\n",
    "    precision = np.zeros(len(results))\n",
    "\n",
    "    if results[0][\"TP\"] == 1:\n",
    "        acc_TP[0] = 1\n",
    "    else:\n",
    "        acc_FP[0] = 1\n",
    "\n",
    "    for i in range(1, len(results)):\n",
    "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
    "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
    "\n",
    "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
    "        recall[i] = acc_TP[i] / nbr_boxes\n",
    "\n",
    "    return auc(recall, precision)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYe2pO1pbNEM"
   },
   "source": [
    "Вам мы оставляем творческую часть =)\n",
    "\n",
    "__Задание__. Обучите модель для object detection на __обучающем__ датасете и добейтесь PR-AUC не менее __0.94__ на  __тестовом__.\n",
    "\n",
    " - Создайте модель и оптимайзер\n",
    " - Напишите функцию обучения модели\n",
    " - Используйте аугментации\n",
    " \n",
    "Использовать аугментации для обучения __обязательно__. Они дадут 1 балл из 5. Пользуйтесь модулем torchvision.transforms или библиотекой albumentations (о которой говорилось ранее). Последняя библиотека особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Советуем обратить внимание на следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). Обратите внимание, что код, написанный в датасете выше, верен только если вы используете albumentations. Если вы выбрали путь torchvision.transforms, вам потребуется метод `__getitem__` изменить (что-то типа `return self.transform(self.images[i])`; однако в таком случае вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную =))\n",
    "\n",
    "Оставшиеся 4 балла вычисляются по простой формуле: __min(4, 4 * (Ваш auc - 0.5) / 0.94)__."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N45-6AgUbNEM"
   },
   "source": [
    "def train_one_epoch(model, train_dataloader, optimizer, device):\n",
    "    # YOUR CODE\n",
    "    # TRAIN YOUR MODEL ON THE train_dataloader\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.eval()\n",
    "        a = evaluate(model, val_dataloader, device=device)\n",
    "        print(\"AUC ON TEST: {.4f}\".format(a))\n",
    "        model.train()\n",
    "        train_one_epoch(model, dataloader, optimizer, device=device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DdjHqoA9bNEN"
   },
   "source": [
    "train_transform = # YOUR CODE FOR AUGMENTATIONS\n",
    "val_transform = # YOUR CODE FOR VALIDATION AUGMENTATIONS\n",
    "# HINT: TRAIN TRANSFORM OBVIOUSLY SHOULD BE HARDER THAN THOSE FOR VALIDATION\n",
    "\n",
    "train_dataset = FruitDataset(\"./train_zip/train\", transform=train_transform)\n",
    "val_dataset = FruitDataset(\"./train_zip/test\", transform=val_transform)\n",
    "\n",
    "model = # YOUR CODE, CREATE MODEL FOR OBJECT DETECTION\n",
    "# HINT: USE MATERIALS FROM THE SEMINAR\n",
    "# YOU CAN USE torchvision.models AND torchvision.models.detection\n",
    "# READ OFFICIAL DOCS FOR MORE INFO\n",
    "\n",
    "optimizer = # SELECT YOUR OPTIMIZER\n",
    "train_dataloader = # CREATE YOUR DATALOADER, SELECT APPROPRIATE batch_size\n",
    "val_dataloader = # CREATE VALIDATION DATALOADER\n",
    "n_epochs = # SELECT APPROPRIZTE NUMBER OF EPOCHS\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_r5tbqibNEN"
   },
   "source": [
    "__Выведите итоговое качество модели__."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EujF8xEQbNEN"
   },
   "source": [
    "auc = evaluate(model, val_dataloader, criterion)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(4, 4 * (auc - 0.5) / 0.94)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prVsvBQFbNEN"
   },
   "source": [
    "Нарисуйте предсказанные bounding box'ы для любых двух картинок из __тестового__ датасета."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LN1hK3-KbNEN"
   },
   "source": [
    "image, labels = next(iter(train_dataset))\n",
    "pred = model(image.unsqueeze(0).to(device))[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mUnFUeqabNEN"
   },
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "image = torchvision.transform.ToPILImage()(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in labels[\"boxes\"]:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "\n",
    "for box in pred[\"boxes\"]:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\")\n",
    "image"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKZJxw1lbNEO"
   },
   "source": [
    "## Бонус (0 баллов).\n",
    "\n",
    "__Задание 1__. Скиньте ниже смешную картинку, желательно про машинное обучение. На картинке не должно быть никаких упоминаний лектора, семинаристов и ассистентов этого курса. Если картинка будет смешной, проверяющему(-ей) будет приятно :3\n",
    "\n",
    "__Задание 2__. Расскажите, как вам курс в целом? Что понравилось, что не понравилось, что можно улучшить? Мы примем во внимание любой фидбек."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4sWGdKqIbvBn"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
