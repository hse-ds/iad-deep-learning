{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2. Классификация, детекция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка за часть 1 и часть 2 в этом дз -- по 5 баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Классификация\n",
    "\n",
    "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в датасете есть. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "Структура датасета простая -- есть директории train и val, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат собственно сами изображения.\n",
    " \n",
    "__Задание__. Добейтесь accuracy **не менее 0.44**. Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи/блогпосты/вопросы на stackoverflow/видосы от (индийских) ютуберов/курсы/подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
    "\n",
    "В коде ниже необходимо, чтобы код проходил все `assert`'ы.\n",
    "\n",
    "Необходимо написать функцию `predict` по шаблону ниже. Эта функция принимает на вход модель, даталоадер с валидационнами данными, criterion для подсчёта лосса и device, на котором будут производиться вычисления (определён ниже) и возвращает список лоссов по всем объектам, список из предсказанных классов для каждого объекта из из даталоалера и список из настоящих классов для каждого объекта в даталоадере (и именно в таком порядке).\n",
    "\n",
    "__Использовать внешние данные для обучения строго запрещено__. Можно использовать предобученные модели из `torchvision`.\n",
    "\n",
    "__Критерии оценки__: Оценка вычисляется по простой формуле: min(5, 5 * Ваша accuracy / 0.44). Оценка округляется до десятых по арифметическим правилам.\n",
    "\n",
    "__Советы и указания__:\n",
    " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать на защите :)\n",
    " - Используйте аугментации. Для этого пользуйтесь модулем torchvision.transforms или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - (ещё раз) Можно файнтюнить предобученные модели из `torchvision`.\n",
    " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом ImageFolder), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
    " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
    " - Чтобы отладить код, пробуйте обучаться на маленькой части датасета. Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
    " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
    " - Фиксируйте random seed.\n",
    " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
    " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
    " - Советуем использовать гпу. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "# You may add any imports you need\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        # YOUR CODE\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE\n",
    "        pass\n",
    "    def __len__(self, idx):\n",
    "        # YOUR CODE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = None\n",
    "val_transform = None\n",
    "# YOU CAN DEFINE AUGMENTATIONS HERE\n",
    "\n",
    "train_dataset = MyDataset(\"./dataset/dataset/train\", transform=train_transform)\n",
    "val_dataset = MyDataset(\"./dataset/dataset/val\", transform=val_transform)\n",
    "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
    "# OR USE torchvision.datasets.ImageFolder INSTEAD OF MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just very simple checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\"):\n",
    "    model.train()\n",
    "    # YOUR CODE\n",
    "    # TRAIN YOUR MODEL HERE\n",
    "    pass\n",
    "\n",
    "def predict(model, val_dataloder, criterion, device=\"cuda:0\"):\n",
    "    model.eval()\n",
    "    # YOUR CODE\n",
    "    # predict val_dataloader and print and return the validation accuracy\n",
    "    # pass\n",
    "    return losses, predicted_classes, true_classes\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\n",
    "    model.to(device)\n",
    "    for epoch in range(n_epochs):\n",
    "        # YOUR CODE\n",
    "        # Train, evaluate, print accuracy, make a step of scheduler or whatever you want...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = # THE MODEL THAT YOU CHOOSE\n",
    "optimizer = # YOUR OPTIMIZER\n",
    "train_dataloader = # TRAIN DATALOADER WHICH YOU CONSTRUCT\n",
    "val_dataloader = # VAL DATALOADER WHICH YOU CONSTRUCT\n",
    "criterion = # LOSS THAT YOU OPTIMIZE (SHOLD BE CROSS ENTROPY OR SMTH ELSE)\n",
    "scheduler = # LR SCHEDULE THAT YOU PROBABLY CHOOSE\n",
    "n_epochs = # NUMBER OF EPOCHS\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простой тест на проверку правильности написанного кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(predicted_labels, true_labels)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустить обучение можно в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После всех экспериментов которые вы проделали, выберите лучшую из своих моделей, запустите функцию evaluate. Эта функция должна брать на вход модель и даталоадер с валидационными данными и возврашать accuracy, посчитанную на этом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(5, 5*accuracy / 0.44)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваш отчёт о проделанных экспериментах__: текст писать тут"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Object detection.\n",
    "\n",
    "В этом задании потребуется обучить детектор фруктов на изображении. Датасет можно скачать [отсюда](https://yadi.sk/d/UPwQB7OZrB48qQ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need this library to process the labeling\n",
    "! pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет мы за вас написали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xmltodict\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "# add any imports you need\n",
    "\n",
    "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
    "\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        self.transform = transform\n",
    "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
    "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
    "            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n",
    "            with open(annotation) as f:\n",
    "                annotation_dict = xmltodict.parse(f.read())\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
    "            if not isinstance(objects, list):\n",
    "                objects = [objects]\n",
    "            for obj in objects:\n",
    "                bndbox = obj[\"bndbox\"]\n",
    "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
    "                bbox = list(map(int, bbox))\n",
    "                bboxes.append(torch.tensor(bbox))\n",
    "                labels.append(class2tag[obj[\"name\"]])\n",
    "            self.annotations.append(\n",
    "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.transform:\n",
    "            # the following code is correct if you use albumentations\n",
    "            # if you use torchvision transforms you have to modify it =)\n",
    "            res = self.transform(\n",
    "                image=self.images[i],\n",
    "                bboxes=self.annotations[i][\"boxes\"],\n",
    "                labels=self.annotations[i][\"labels\"],\n",
    "            )\n",
    "            return res[\"image\"], {\n",
    "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
    "                \"labels\": torch.tensor(res[\"labels\"]),\n",
    "            }\n",
    "        else:\n",
    "            return self.images[i], self.annotations[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выпишем кое-какую техническую работу, которая уже была на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(dt_bbox, gt_bbox):\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO YOUR CODE\n",
    "\n",
    "    intersection_bbox = np.array(\n",
    "        [\n",
    "            max(dt_bbox[0], gt_bbox[0]),\n",
    "            max(dt_bbox[1], gt_bbox[1]),\n",
    "            min(dt_bbox[2], gt_bbox[2]),\n",
    "            min(dt_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
    "        intersection_bbox[3] - intersection_bbox[1], 0\n",
    "    )\n",
    "    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "\n",
    "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
    "    gt_bboxes = target_true[\"boxes\"].numpy()\n",
    "    gt_labels = target_true[\"labels\"].numpy()\n",
    "\n",
    "    dt_bboxes = target_pred[\"boxes\"].numpy()\n",
    "    dt_labels = target_pred[\"labels\"].numpy()\n",
    "    dt_scores = target_pred[\"scores\"].numpy()\n",
    "\n",
    "    results = []\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "\n",
    "        detection_result_dict = {\"score\": dt_score}\n",
    "\n",
    "        max_IoU = 0\n",
    "        max_gt_id = -1\n",
    "        for gt_id in range(len(gt_labels)):\n",
    "            gt_bbox = gt_bboxes[gt_id, :]\n",
    "            gt_label = gt_labels[gt_id]\n",
    "\n",
    "            if gt_label != dt_label:\n",
    "                continue\n",
    "\n",
    "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
    "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
    "                max_gt_id = gt_id\n",
    "\n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            detection_result_dict[\"TP\"] = 1\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict[\"TP\"] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, targets_true) in enumerate(test_loader):\n",
    "            images = list(image.to(device).float() for image in images)\n",
    "            targets_pred = model(images)\n",
    "            targets_true = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n",
    "            ]\n",
    "            targets_pred = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n",
    "            ]\n",
    "\n",
    "            for i in range(len(targets_true)):\n",
    "                target_true = targets_true[i]\n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
    "\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "\n",
    "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "    acc_TP = np.zeros(len(results))\n",
    "    acc_FP = np.zeros(len(results))\n",
    "    recall = np.zeros(len(results))\n",
    "    precision = np.zeros(len(results))\n",
    "\n",
    "    if results[0][\"TP\"] == 1:\n",
    "        acc_TP[0] = 1\n",
    "    else:\n",
    "        acc_FP[0] = 1\n",
    "\n",
    "    for i in range(1, len(results)):\n",
    "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
    "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
    "\n",
    "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
    "        recall[i] = acc_TP[i] / nbr_boxes\n",
    "\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам мы оставляем творческую часть =)\n",
    "\n",
    "__Задание__. Обучите модель для object detection на __обучающем__ датасете и добейтесь PR-AUC не менее __0.91__ на  __тестовом__.\n",
    "\n",
    " - Создайте модель и оптимайзер\n",
    " - Напишите функцию обучения модели\n",
    " - Используйте аугментации\n",
    " \n",
    "Использовать аугментации для обучения __обязательно__. Они дадут 1 балл из 5. Пользуйтесь модулем torchvision.transforms или библиотекой albumentations (о которой говорилось ранее). Последняя библиотека особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Советуем обратить внимание на следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). Обратите внимание, что код, написанный в датасете выше, верен только если вы используете albumentations. Если вы выбрали путь torchvision.transforms, вам потребуется метод `__getitem__` изменить (что-то типа `return self.transform(self.images[i])`; однако в таком случае вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную =))\n",
    "\n",
    "Оставшиеся 4 балла вычисляются по простой формуле: __min(4, 4 * Ваш auc / 0.91)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, optimizer, device):\n",
    "    # YOUR CODE\n",
    "    # TRAIN YOUR MODEL ON THE train_dataloader\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.eval()\n",
    "        a = evaluate(model, val_dataloader, device=device)\n",
    "        print(\"AUC ON TEST: {.4f}\".format(a))\n",
    "        model.train()\n",
    "        train_one_epoch(model, dataloader, optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = # YOUR CODE FOR AUGMENTATIONS\n",
    "val_transform = # YOUR CODE FOR VALIDATION AUGMENTATIONS\n",
    "# HINT: TRAIN TRANSFORM OBVIOUSLY SHOULD BE HARDER THAN THOSE FOR VALIDATION\n",
    "\n",
    "train_dataset = FruitDataset(\"./train_zip/train\", transform=train_transform)\n",
    "val_dataset = FruitDataset(\"./train_zip/test\", transform=val_transform)\n",
    "\n",
    "model = # YOUR CODE, CREATE MODEL FOR OBJECT DETECTION\n",
    "# HINT: USE MATERIALS FROM THE SEMINAR\n",
    "# YOU CAN USE torchvision.models AND torchvision.models.detection\n",
    "# READ OFFICIAL DOCS FOR MORE INFO\n",
    "\n",
    "optimizer = # SELECT YOUR OPTIMIZER\n",
    "train_dataloader = # CREATE YOUR DATALOADER, SELECT APPROPRIATE batch_size\n",
    "val_dataloader = # CREATE VALIDATION DATALOADER\n",
    "n_epochs = # SELECT APPROPRIZTE NUMBER OF EPOCHS\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Выведите итоговое качество модели__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = evaluate(model, val_dataloader, criterion)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(4, 4 * auc / 0.91)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте предсказанные bounding box'ы для любых двух картинок из __тестового__ датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, labels = next(iter(train_dataset))\n",
    "pred = model(image.unsqueeze(0).to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "image = torchvision.transform.ToPILImage()(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in labels['boxes']:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "    \n",
    "for box in pred['boxes']:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус (10 баллов).\n",
    "\n",
    "__Задание__. В части с классификацией добейтесь выполнения хотя бы одного из перечисленных критериев:\n",
    " - Достичь accuracy не менее 0.56, **не используя resize картинок**, а также **не используя предобученные сети**;\n",
    " - Достичь accuracy не менее 0.86, но использовать resize и предобученные из torchvision сети разрешается. \n",
    " \n",
    "Напишите отчёт о проделанных экспериментах.\n",
    "\n",
    "__Критерии оценки__. Оценка за бонусную часть равна 10, если вы выполнили хоть один из вышеперечисленных критериев и 0 в противном случае.\n",
    "\n",
    "__Иных оценок кроме 0 и 10 не предусмотрено__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус (0 баллов).\n",
    "\n",
    "__Задание 1__. Скиньте ниже смешную картинку, желательно про машинное обучение. На картинке не должно быть никаких упоминаний лектора, семинаристов и ассистентов этого курса.\n",
    "\n",
    "__Задание 2__. Расскажите, как вам задание? Что понравилось, что не понравилось, что можно улучшить? Мы примем во внимание любой фидбек."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}